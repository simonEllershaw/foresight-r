{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Inference Analysis Notebook\n",
    "\n",
    "This notebook analyzes the inference outputs from `outputs/inference/train/20260204_173740/ed_reattendance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = [10, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"outputs/inference/train/20260204_173740/ed_reattendance/0.parquet\"\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "# Ensure correct types\n",
    "df[\"label\"] = df[\"boolean_value\"].astype(int)\n",
    "df[\"prob\"] = pd.to_numeric(df[\"parsed_probability\"], errors=\"coerce\")\n",
    "\n",
    "# Normalize probability if needed\n",
    "if df[\"prob\"].max() > 1.0:\n",
    "    print(\"Detected probabilities > 1, normalizing by dividing by 100.\")\n",
    "    df[\"prob\"] = df[\"prob\"] / 100.0\n",
    "\n",
    "print(f\"Loaded {len(df)} rows.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Visualize Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df[:3].iterrows():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Row {idx}\")\n",
    "    print(f\"Prompt: {row['prompt']}\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Model Output:\\n{row['model_output'][:500]}...\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Metrics with Bootstrapped Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_prob):\n",
    "    return {\n",
    "        \"AUROC\": roc_auc_score(y_true, y_prob),\n",
    "        \"AUPRC\": average_precision_score(y_true, y_prob),\n",
    "        \"Brier\": brier_score_loss(y_true, y_prob),\n",
    "    }\n",
    "\n",
    "\n",
    "def bootstrap_metrics(y_true, y_prob, n_bootstrap=1000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    metrics_list = []\n",
    "\n",
    "    base_metrics = calculate_metrics(y_true, y_prob)\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = resample(np.arange(len(y_true)), replace=True)\n",
    "        y_true_boot = y_true[indices]\n",
    "        y_prob_boot = y_prob[indices]\n",
    "\n",
    "        try:\n",
    "            metrics_list.append(calculate_metrics(y_true_boot, y_prob_boot))\n",
    "        except ValueError:\n",
    "            # Handle cases where bootstrap sample has only one class\n",
    "            continue\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "    results = {}\n",
    "    for metric in base_metrics:\n",
    "        lower = np.percentile(metrics_df[metric], 2.5)\n",
    "        upper = np.percentile(metrics_df[metric], 97.5)\n",
    "        results[metric] = {\"value\": base_metrics[metric], \"95% CI\": (lower, upper)}\n",
    "    return results\n",
    "\n",
    "\n",
    "# Filter out NaNs and validate bounds\n",
    "valid_df = df.dropna(subset=[\"label\", \"prob\"])\n",
    "y_true = valid_df[\"label\"].values\n",
    "y_prob = valid_df[\"prob\"].values\n",
    "\n",
    "# Ensure probabilities are valid\n",
    "y_prob = np.clip(y_prob, 0, 1)\n",
    "\n",
    "results = bootstrap_metrics(y_true, y_prob)\n",
    "\n",
    "for metric, data in results.items():\n",
    "    print(\n",
    "        f\"{metric}: {data['value']:.4f} (95% CI: {data['95% CI'][0]:.4f} - {data['95% CI'][1]:.4f})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4. Plots with Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve_with_bootstrap(\n",
    "    y_true, y_prob, curve_type=\"roc\", n_bootstrap=100, ax=None\n",
    "):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    if curve_type == \"roc\":\n",
    "        base_x, base_y, _ = roc_curve(y_true, y_prob)\n",
    "        ax.plot(base_x, base_y, color=\"blue\", label=\"Actual\", linewidth=2)\n",
    "        ax.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "        ax.set_xlabel(\"False Positive Rate\")\n",
    "        ax.set_ylabel(\"True Positive Rate\")\n",
    "        ax.set_title(\"ROC Curve\")\n",
    "\n",
    "        # Bootstrap for shading\n",
    "        tprs = []\n",
    "        base_fpr = np.linspace(0, 1, 101)\n",
    "\n",
    "        for _ in range(n_bootstrap):\n",
    "            indices = resample(np.arange(len(y_true)), replace=True)\n",
    "            if len(np.unique(y_true[indices])) < 2:\n",
    "                continue\n",
    "            fpr, tpr, _ = roc_curve(y_true[indices], y_prob[indices])\n",
    "            tpr_interp = np.interp(base_fpr, fpr, tpr)\n",
    "            tpr_interp[0] = 0.0\n",
    "            tprs.append(tpr_interp)\n",
    "\n",
    "        tprs = np.array(tprs)\n",
    "        mean_tprs = tprs.mean(axis=0)\n",
    "        std_tprs = tprs.std(axis=0)\n",
    "        tprs_upper = np.minimum(mean_tprs + 2 * std_tprs, 1)\n",
    "        tprs_lower = np.maximum(mean_tprs - 2 * std_tprs, 0)\n",
    "\n",
    "        ax.fill_between(\n",
    "            base_fpr, tprs_lower, tprs_upper, color=\"blue\", alpha=0.2, label=\"95% CI\"\n",
    "        )\n",
    "\n",
    "    elif curve_type == \"prc\":\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "        ax.plot(recall, precision, color=\"green\", label=\"Actual\", linewidth=2)\n",
    "        ax.set_xlabel(\"Recall\")\n",
    "        ax.set_ylabel(\"Precision\")\n",
    "        ax.set_title(\"Precision-Recall Curve\")\n",
    "\n",
    "        # Bootstrap for shading - interpolation is trickier for PRC, simpler approach: plot many lines\n",
    "        for _ in range(\n",
    "            min(n_bootstrap, 50)\n",
    "        ):  # Limit lines for PRC to avoid clutter if not interpolating\n",
    "            indices = resample(np.arange(len(y_true)), replace=True)\n",
    "            if len(np.unique(y_true[indices])) < 2:\n",
    "                continue\n",
    "            p, r, _ = precision_recall_curve(y_true[indices], y_prob[indices])\n",
    "            ax.plot(r, p, color=\"green\", alpha=0.05)\n",
    "\n",
    "    elif curve_type == \"calibration\":\n",
    "        prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=10)\n",
    "        ax.plot(prob_pred, prob_true, marker=\"o\", label=\"Actual\", color=\"red\")\n",
    "        ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "        ax.set_xlabel(\"Mean Predicted Probability\")\n",
    "        ax.set_ylabel(\"Fraction of Positives\")\n",
    "        ax.set_title(\"Calibration Curve\")\n",
    "\n",
    "        # Bootstrap\n",
    "        for _ in range(min(n_bootstrap, 50)):\n",
    "            indices = resample(np.arange(len(y_true)), replace=True)\n",
    "            if len(np.unique(y_true[indices])) < 2:\n",
    "                continue\n",
    "            pt, pp = calibration_curve(y_true[indices], y_prob[indices], n_bins=10)\n",
    "            ax.plot(pp, pt, color=\"red\", alpha=0.1)\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "plot_curve_with_bootstrap(y_true, y_prob, \"roc\", ax=axes[0])\n",
    "plot_curve_with_bootstrap(y_true, y_prob, \"prc\", ax=axes[1])\n",
    "plot_curve_with_bootstrap(y_true, y_prob, \"calibration\", ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
