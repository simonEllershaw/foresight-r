defaults:
  - ../_defaults@_here_
  - _self_

# Zero-shot LLM inference specific settings

# Generation settings
# https://huggingface.co/Qwen/Qwen3-0.6B#best-practices
generation:
  max_new_tokens: 128
  do_sample: true
  use_cache: true
  temperature: 0.7
  top_k: 20
  top_p: 0.8
  min_p: 0
